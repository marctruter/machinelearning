{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cfd707de-88fa-4b93-81bf-4b11f2d874f2",
      "metadata": {
        "id": "cfd707de-88fa-4b93-81bf-4b11f2d874f2"
      },
      "source": [
        "# Efficient Loading and Fine-Tuning of Large Language Models\n",
        "\n",
        "In this notebook, we will explore techniques to efficiently load and fine-tune Large Language Models (LLMs) using Parameter-Efficient Fine-Tuning (PEFT) methods. These approaches are essential for optimizing memory usage and computational efficiency, especially when working with limited hardware resources."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install trl\n",
        "#%pip install -U bitsandbytes\n",
        "#%pip install evaluate"
      ],
      "metadata": {
        "id": "WOKQzamDG3h7"
      },
      "id": "WOKQzamDG3h7",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc5601e5-3041-438b-913e-beac79929790",
      "metadata": {
        "id": "fc5601e5-3041-438b-913e-beac79929790"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        "    TrainingArguments\n",
        ")\n",
        "from trl import SFTTrainer, get_kbit_device_map\n",
        "from datasets import load_dataset\n",
        "from peft import get_peft_model, LoraConfig, AutoPeftModelForCausalLM\n",
        "\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b48f4125-ab13-483b-81cb-60c47000b3b0",
      "metadata": {
        "id": "b48f4125-ab13-483b-81cb-60c47000b3b0"
      },
      "outputs": [],
      "source": [
        "# Utility function to get model size:\n",
        "def get_model_size(model):\n",
        "    model_size = 0\n",
        "    for param in model.parameters():\n",
        "        model_size += param.nelement() * param.element_size()\n",
        "    for buffer in model.buffers():\n",
        "        model_size += buffer.nelement() * buffer.element_size()\n",
        "    return model_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "#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"
      ],
      "metadata": {
        "id": "28rVqI3eIGID"
      },
      "id": "28rVqI3eIGID",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "daee7f1f-3f2d-4019-8fb2-2bdff18b93bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daee7f1f-3f2d-4019-8fb2-2bdff18b93bc",
        "outputId": "e48e8118-c231-498a-bfc0-c14bb1de5acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define the model name\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "tokenizer_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Load tokenizer once\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cf19f354-669f-4fb6-a5b7-47597a0d3ffd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf19f354-669f-4fb6-a5b7-47597a0d3ffd",
        "outputId": "a119befa-87fa-4866-8fa4-38a047511955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bf16 model GPU VRAM usage: 4.60 GB\n"
          ]
        }
      ],
      "source": [
        "# Load model in 16-bit quantized mode\n",
        "model_bf16 = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "print(f\"bf16 model GPU VRAM usage: {get_model_size(model_bf16)/1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ec96aa21-52bc-433a-85a0-21fd43fb3ca6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec96aa21-52bc-433a-85a0-21fd43fb3ca6",
        "outputId": "ad10da2f-7357-4b86-aefa-a6f4f257a5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Write a poem on a supercomputer missing its GPUs? (Solve the riddle!)\\nToday is the 25th day of the year. So, this is the 25th day of the 25th day of the 25th day of the 25th day of the 25th'}]\n",
            "Inference time: 3.56 seconds\n"
          ]
        }
      ],
      "source": [
        "# Generate a response for each model\n",
        "for model in [model_bf16]:\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    start_time = time.time()\n",
        "    response = pipe(\n",
        "        \"Write a poem on a supercomputer missing its GPUs?\",\n",
        "        max_new_tokens=50,\n",
        "        batch_size=32,\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    print(response)\n",
        "    print(f\"Inference time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "47754b40-2661-42ad-820c-617019342986",
      "metadata": {
        "id": "47754b40-2661-42ad-820c-617019342986"
      },
      "outputs": [],
      "source": [
        "# Free GPU memory for what we are going to do next\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50495c9-e423-44a7-a96b-54733e5f3fc4",
      "metadata": {
        "id": "a50495c9-e423-44a7-a96b-54733e5f3fc4"
      },
      "source": [
        "## Create a conversational dataset\n",
        "Here, we use `orca-math-word-problems-200k`, which is a set of 200K math problems. This dataset needs to be formatted in a proper \"chat\" format, which is made of a series of roles and their corresponding text. The roles are:\n",
        "- **system**: it's the role where the system prompt, so the instruction regarding how the model should \"behave\" go;\n",
        "- **user**: it's the role corresponding to an user input;\n",
        "- **assistant**: this is the role corresponding to the LLM itself. Here, we put either previous LLM responses, or just blank, since the model still needs to repond.\n",
        "\n",
        "In order to create a dataset of \"chats\", we apply a simple function using the `map` method from `datasets` to our whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cf68af6a-e80e-4125-b7fd-be852d724f45",
      "metadata": {
        "id": "cf68af6a-e80e-4125-b7fd-be852d724f45"
      },
      "outputs": [],
      "source": [
        "# Create system prompt\n",
        "system_message = \"\"\"Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\n",
        "\n",
        "Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\n",
        "\n",
        "# Steps\n",
        "\n",
        "1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\n",
        "2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\n",
        "3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\n",
        "4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\n",
        "5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\n",
        "\n",
        "# Notes\n",
        "\n",
        "- Always clearly define any variable or term used.\n",
        "- Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\n",
        "- Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\n",
        "\"\"\"\n",
        "\n",
        "# convert to messages\n",
        "def create_conversation(sample):\n",
        "  return {\n",
        "    \"messages\": [\n",
        "      {\"role\": \"system\", \"content\": system_message},\n",
        "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
        "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
        "    ]\n",
        "  }\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\n",
        "    \"microsoft/orca-math-word-problems-200k\",\n",
        "    split=\"train\")\n",
        "\n",
        "# Convert dataset to OAI messages\n",
        "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
        "\n",
        "train_dataset = dataset.select(range(10000))\n",
        "test_dataset = dataset.select(range(10001, 12000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1cf09f02-e82d-4c9e-8c1d-43297b90f7b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cf09f02-e82d-4c9e-8c1d-43297b90f7b0",
        "outputId": "965264fd-fb93-47b6-d6c5-ff7909083776"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\\n \\nProvide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\\n \\n# Steps\\n \\n1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\\n2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\\n3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\\n4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\\n5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\\n \\n# Notes\\n \\n- Always clearly define any variable or term used.\\n- Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\\n- Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\\n',\n",
              "   'role': 'system'},\n",
              "  {'content': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n",
              "   'role': 'user'},\n",
              "  {'content': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Example data\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9a556c1-d236-4457-be67-28cf3f8546c6",
      "metadata": {
        "id": "c9a556c1-d236-4457-be67-28cf3f8546c6"
      },
      "source": [
        "## LoRA parameters\n",
        "As discussed during class, a way to optimize training is not to do a full fine-tuning, but do parameter efficient fine-tuning, and use adapters and train just them, reducing of more than 70% the parameters to train.\n",
        "\n",
        "The most used way to do this PEFT with adapters is using the peft library and their implementation of the Low-Rank Adapters (LoRA) paper.\n",
        "\n",
        "Don't worry if you do not understand anything regarding the LoRA parameters. This part will be explained in great detail in the next module. At this stage, take it from granted, and you will understand it better in the future.\n",
        "\n",
        "Steps:\n",
        "- define the configuration of the adapters\n",
        "- load (a quantized?) model\n",
        "- apply the adapters to the model\n",
        "- define training arguments\n",
        "- start a supervised fine-tuning training session and train the model\n",
        "- save the newly trained adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4bf31af0-cc2e-47ba-9d39-dc919a2d8c10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bf31af0-cc2e-47ba-9d39-dc919a2d8c10",
        "outputId": "89280ebd-fc31-4705-9d3f-24954244e0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 547,880,960 || all params: 1,783,695,360 || trainable%: 30.7161\n",
            "None\n",
            "Applied LoRA. Model is now ready for fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA with desired hyperparameters\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    # important as we need to train the special tokens for the chat template of llama\n",
        "    # you might need to change this for qwen or other models\n",
        "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply PEFT with LoRA\n",
        "model_bf16 = get_peft_model(model_bf16, lora_config).to(\"cuda:0\")\n",
        "print(model_bf16.print_trainable_parameters())\n",
        "print(\"Applied LoRA. Model is now ready for fine-tuning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4b9aaf22-cd28-4ab9-ae33-d81c6468b954",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "4b9aaf22-cd28-4ab9-ae33-d81c6468b954",
        "outputId": "6ebd3701-bd4f-42b9-fbfa-df1b5eef300d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 01:28, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.177400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.579400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.530200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.496700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_finetune\", # directory to save and repository id\n",
        "    # num_train_epochs=5, # number of training epochs\n",
        "    per_device_train_batch_size=1, # batch size per device during training\n",
        "    gradient_accumulation_steps=1, # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True, # use gradient checkpointing to save memory\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False},  # Use newer implementation that will become the default.\n",
        "    optim=\"adamw_torch_fused\", # use fused adamw optimizer\n",
        "    logging_steps=10, # log every 10 steps\n",
        "    save_strategy='no',  # 'no', 'epoch' or 'steps'\n",
        "    logging_strategy='steps',  # 'no', 'epoch' or 'steps'\n",
        "    learning_rate=2e-4, # learning rate, based on QLoRA paper\n",
        "    bf16=True, # use bfloat16 precision\n",
        "    max_grad_norm=0.3, # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03, # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\", # use constant learning rate scheduler\n",
        "    max_steps=40,\n",
        "    report_to='none',  # disable wandb\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model_bf16,\n",
        "    train_dataset=train_dataset,\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c0830155-863b-4f5a-9610-de65c09e0abc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0830155-863b-4f5a-9610-de65c09e0abc",
        "outputId": "b99d977e-f72a-45df-bcaf-c49f4fd69807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  total_flos               =   155961GF\n",
            "  train_loss               =     0.6959\n",
            "  train_runtime            = 0:01:31.36\n",
            "  train_samples            =      10000\n",
            "  train_samples_per_second =      0.438\n",
            "  train_steps_per_second   =      0.438\n"
          ]
        }
      ],
      "source": [
        "# log metrics\n",
        "metrics = train_result.metrics\n",
        "metrics['train_samples'] = len(train_dataset)\n",
        "trainer.log_metrics('train', metrics)\n",
        "trainer.save_metrics('train', metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "13faece3-a24d-4b98-8dc6-3f2b3b55be42",
      "metadata": {
        "id": "13faece3-a24d-4b98-8dc6-3f2b3b55be42"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427acccd-f4eb-4f1f-86fd-f881ad22d013",
      "metadata": {
        "id": "427acccd-f4eb-4f1f-86fd-f881ad22d013"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "59775d98-be35-4d0d-8e6d-2f25fedc2520",
      "metadata": {
        "id": "59775d98-be35-4d0d-8e6d-2f25fedc2520"
      },
      "outputs": [],
      "source": [
        "# Load the evaluation metrics\n",
        "bleu = evaluate.load('bleu')\n",
        "#rouge = evaluate.load('rouge')\n",
        "#bertscore = evaluate.load('bertscore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a91ec6c8-1e87-411c-8919-7fc3e60f053d",
      "metadata": {
        "id": "a91ec6c8-1e87-411c-8919-7fc3e60f053d"
      },
      "outputs": [],
      "source": [
        "model=model_bf16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cedc2fb8-853c-4080-9c11-027ad5328525",
      "metadata": {
        "id": "cedc2fb8-853c-4080-9c11-027ad5328525"
      },
      "outputs": [],
      "source": [
        "# Function to format input using the system prompt\n",
        "def format_input(system_message, user_prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Function to generate a model response\n",
        "def generate_response(system_message, user_prompt):\n",
        "    input_text = format_input(system_message, user_prompt)\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_length=512)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d667fc8c-02eb-4349-beba-0621bfef0475",
      "metadata": {
        "id": "d667fc8c-02eb-4349-beba-0621bfef0475"
      },
      "outputs": [],
      "source": [
        "# Evaluation dataset (test split)\n",
        "eval_samples = test_dataset.select(range(100))  # Evaluate on a subset of 100 samples\n",
        "\n",
        "references = []\n",
        "predictions = []\n",
        "\n",
        "# Generate predictions\n",
        "for sample in eval_samples:\n",
        "    system_prompt = system_message\n",
        "    user_input = sample[\"messages\"][1][\"content\"]  # User's question\n",
        "    reference_output = sample[\"messages\"][2][\"content\"]  # Expected assistant response\n",
        "\n",
        "    predicted_output = generate_response(system_prompt, user_input)\n",
        "\n",
        "    references.append(reference_output)\n",
        "    predictions.append(predicted_output)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "# Print results\n",
        "print(f\"BLEU Score: {bleu_result['bleu']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfWMO9YCeq47"
      },
      "id": "wfWMO9YCeq47",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}