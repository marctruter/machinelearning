{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arnJL1fjbhLi"
      },
      "source": [
        "## Talk 8: score-based diffusion models\n",
        "\n",
        "> **Note**: To run this notebook you'll also need to install PyTorch and `tqdm` using Pip. See previous notebooks for the command needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pztl-JSMvp1O"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "from jax.tree_util import tree_map\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "from flax import struct\n",
        "import optax\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHNY06kWXWRI"
      },
      "source": [
        "First, we'll generate a data set in $X = \\mathbb{R}^{2}$ with two clusters, given by the Gaussian mixture density\n",
        "\n",
        "$$ \\Upsilon(x) = \\frac{1}{3\\sqrt{2\\pi}} \\exp\\left(\\frac{\\|x - \\mu_{1}\\|^{2}}{2}\\right) + \\frac{2}{3\\sqrt{2\\pi}} \\exp\\left(\\frac{\\|x - \\mu_{2}\\|^{2}}{2}\\right) $$\n",
        "\n",
        "with $\\mu_{1} = (-10, 1)$ and $\\mu_{2} = (10, 0)$.\n",
        "We do this by generating $100$ points from the first Gaussian density and $200$ from the second (reflecting the $1/3$ to $2/3$ ratio in the density).\n",
        "\n",
        "We'll then plot samples from $\\Upsilon$ and its noised version $\\Upsilon_{\\sigma}$ with $\\sigma = 3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "7rVzYZH_vvA4",
        "outputId": "2c2a75b9-011a-4ac3-aa78-32e71fb5258b"
      },
      "outputs": [],
      "source": [
        "# Generate Gaussian mixure in R^2\n",
        "\n",
        "SIGMA = 3\n",
        "\n",
        "N_DATAPOINTS_1 = 100\n",
        "MU_1 = np.array([-10, 1])\n",
        "\n",
        "N_DATAPOINTS_2 = 200\n",
        "MU_2 = np.array([10, 0])\n",
        "\n",
        "X = np.vstack(\n",
        "    [\n",
        "        (MU_1 + np.random.randn(N_DATAPOINTS_1, 2)),\n",
        "        (MU_2 + np.random.randn(N_DATAPOINTS_2, 2)),\n",
        "    ]\n",
        ")\n",
        "plt.scatter(X[:, 0], X[:, 1], label=r\"$\\Upsilon$\")\n",
        "plt.xlim([-15, 15])\n",
        "plt.ylim([-15, 15])\n",
        "\n",
        "X_noised = X + np.random.randn(*X.shape) * SIGMA\n",
        "plt.scatter(X_noised[:, 0], X_noised[:, 1], label=r\"$\\Upsilon_{\\sigma}$\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0clcfykYX_S"
      },
      "source": [
        "Now we'll convert this data set into a form that is easier to work with for `jax`.\n",
        "\n",
        "**Technical details**: we use PyTorch's well-developed data loader code to handle shuffling and batching of data ready for use in the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPGeujrJzw8D"
      },
      "outputs": [],
      "source": [
        "# `_numpy_collate` and `NumpyLoader` are based on the JAX notebook https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
        "def _numpy_collate(batch):\n",
        "    return tree_map(np.asarray, default_collate(batch))\n",
        "\n",
        "\n",
        "class NumpyLoader(DataLoader):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        sampler=None,\n",
        "        batch_sampler=None,\n",
        "        num_workers=0,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        timeout=0,\n",
        "        worker_init_fn=None,\n",
        "    ):\n",
        "\n",
        "        # Batch_sampler option is mutually exclusive with\n",
        "        # batch_size, shuffle, sampler, and drop_last.\n",
        "        if batch_sampler is not None:\n",
        "            additional_args = {}\n",
        "        else:\n",
        "            additional_args = {\n",
        "                \"batch_size\": batch_size,\n",
        "                \"shuffle\": shuffle,\n",
        "                \"drop_last\": drop_last,\n",
        "            }\n",
        "\n",
        "        super(self.__class__, self).__init__(\n",
        "            dataset,\n",
        "            sampler=sampler,\n",
        "            batch_sampler=batch_sampler,\n",
        "            num_workers=num_workers,\n",
        "            collate_fn=_numpy_collate,\n",
        "            pin_memory=pin_memory,\n",
        "            timeout=timeout,\n",
        "            worker_init_fn=worker_init_fn,\n",
        "            **additional_args,\n",
        "        )\n",
        "\n",
        "\n",
        "train_dataloader = NumpyLoader(X, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppMKhzTXYuYS"
      },
      "source": [
        "Now we'll set up the architecture for the score network $s_{\\theta} \\colon \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$. We'll use a simple neural network with two hidden layers of width 256."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-w0uXcZZBJF"
      },
      "outputs": [],
      "source": [
        "class ScoreNetwork(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=256)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(features=256)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(features=2)(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG-NsEdAZEOW"
      },
      "source": [
        "Next we'll set up the loss function (eq. (13.22) in https://arxiv.org/pdf/2410.10523v1), and a helper function which takes care of stochastic gradient descent for us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nDIt5mZwFmH"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, batch, key):\n",
        "    # LOSS FUNCTION IS DEFINED HERE\n",
        "    def loss_fn(params):\n",
        "        noised_batch = batch + SIGMA * jax.random.normal(key)\n",
        "        score = state.apply_fn({\"params\": params}, noised_batch)\n",
        "        return jnp.sum((score + (noised_batch - batch) / SIGMA**2) ** 2)\n",
        "\n",
        "    # Computes the value of the loss function, and the gradient with respect to\n",
        "    # the parameters \\theta.\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "\n",
        "    # Uses the stochastic gradient descent (SGD) optimiser defined below to apply\n",
        "    # the gradient update to the parameters \\theta.\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss\n",
        "\n",
        "\n",
        "def create_train_state(module, rng, learning_rate):\n",
        "    params = module.init(rng, jnp.ones([1, 2]))[\n",
        "        \"params\"\n",
        "    ]  # initialize parameters by passing a template image\n",
        "    tx = optax.sgd(learning_rate)\n",
        "    return train_state.TrainState.create(apply_fn=module.apply, params=params, tx=tx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW-E33sJZq8T"
      },
      "source": [
        "Now we'll initialise the score network, create the training state, and train for 1000 epochs (i.e., 1000 full loops over the training data). We use the `tqdm` package to get a nice-looking progress bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbhAMdF7Zo77",
        "outputId": "d671dae9-88d5-4538-9eb7-e3e33c90924a"
      },
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "s_theta = ScoreNetwork()\n",
        "state = create_train_state(s_theta, key, learning_rate=1e-5)\n",
        "\n",
        "EPOCHS = 1000\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for step, batch in enumerate((pbar := tqdm(iter(train_dataloader)))):\n",
        "        key, _ = jax.random.split(key)\n",
        "        state, loss = train_step(state, batch, key)\n",
        "        pbar.set_postfix_str(f\"loss: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Uuou9XaAzb"
      },
      "source": [
        "Now we use an Euler--Maruyama discretisation of the Langevin dynamics (eq (13.17) of https://arxiv.org/pdf/2410.10523v1) to draw samples from $\\Upsilon_{\\sigma}$.\n",
        "\n",
        "The blue scatter points are true samples from $\\Upsilon_{\\sigma}$; the orange scatter points are independent realisations from Langevin dynamics truncated at a finite time step, with Gaussian initial condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "lstfH8rfARSZ",
        "outputId": "84ed9968-311d-4068-a2bc-0dad1cb202e9"
      },
      "outputs": [],
      "source": [
        "# Sample using Langevin dynamics\n",
        "\n",
        "def langevin_sample(key, x0, params, step, n_steps):\n",
        "  history = []\n",
        "  @jax.jit\n",
        "  def _inner(key, x):\n",
        "    output = state.apply_fn({'params': params}, x)\n",
        "    return x + (step / 2) * output + jnp.sqrt(step) * jax.random.normal(key, x.shape)\n",
        "\n",
        "  history = [x0]\n",
        "  for i in range(0, n_steps):\n",
        "    key, _ = jax.random.split(key)\n",
        "    history.append(_inner(key, history[-1]))\n",
        "  return history\n",
        "\n",
        "N_SAMPLES = 100\n",
        "x0 = np.random.randn(N_SAMPLES, 2)\n",
        "\n",
        "x_sampled = langevin_sample(key, x0, state.params, 0.01, 10000)\n",
        "plt.scatter(X_noised[:,0], X_noised[:,1],label='$\\Upsilon_{\\sigma}$')\n",
        "plt.scatter(x_sampled[-1][:,0], x_sampled[-1][:, 1], label='samples from generative model')\n",
        "plt.xlim([-20, 20])\n",
        "plt.ylim([-20, 20])\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHofzbGJaLOf"
      },
      "source": [
        "**Conclusions**: there is reasonable agreement between the two distributions, but the Langevin dynamicis samples seem to have much higher probability of staying near the origin. This is because there is very little data around $(0, 0)$, so estimates of the score are not that accurate.\n",
        "\n",
        "The fix for this is described in the paper https://arxiv.org/pdf/1907.05600, which had significant impact in the machine-learning community. Their solution is to perform denoising score matching *at multiple noise scales simultaneously*, and then use *annealed Langevin dynamics* (start at high noise level and slowly decrease it when generating samples).\n",
        "\n",
        "This then evolved into the paper https://arxiv.org/pdf/2011.13456, which can be essentially viewed as doing denoising score matching at infinitely many noise scales at once. These tricks are **essential** to get good performance, but we don't cover them in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-MJBH_tbRxF"
      },
      "source": [
        "To illustrate the Langevin dynamics in more detail, we follow the path of a single simulation of Langevin dynamics starting at the origin, starting with black points at time $0$ and ending with red points at the final time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "G32-C9x8BSU6",
        "outputId": "02bceb7a-f831-42e1-c612-1ac233459f77"
      },
      "outputs": [],
      "source": [
        "x0 = np.zeros((1, 2))\n",
        "plot_frequency = 1000\n",
        "n_steps = 100000\n",
        "x_sampled = langevin_sample(key, x0, state.params, 0.001, n_steps)\n",
        "\n",
        "plt.scatter(X_noised[:, 0], X_noised[:, 1])\n",
        "for i, x in enumerate(x_sampled):\n",
        "    if i % plot_frequency != 0:\n",
        "        continue\n",
        "    plt.scatter(x[:, 0], x[:, 1], c=(i / n_steps, 0, 0))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
